---
title: "Statistic Model to Analyze Student’s Performance - Group 8"
author: "Xuchuan Zheng, Sungki Park, Prashant Sharma"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r eval=TRUE, echo=FALSE}
# Library
library (stringr)
library(olsrr)
library(ggplot2)
library(lmtest)
library(mctest)
```

# 1. _Introduction_

In out final project for Data 603 - Statistical Modelling with Data, we have tried to develop a model to analyze the impact of various demographic and social factors on the performance of students. Academic performance, though it is not the only factor but is one of the crucial factors in shaping a student's future. To get into a good collage/university, student must score grades in school, a good college ca lead a better future and economic stability. So in order to secure good grades, getting into a great school is enough? Is there something more than a great school that can help a student to perform better? Do the social and demographic factors plays any role in student's performance? In our project we are trying to answer these question.

To answer these questions we are working with a dataset that is collected at 2 Portuguese schools for Mathematics and Portuguese subject. This data is collected by using school reports and questionnaires. The data attribute include students grades, family size information, education level of parents, free time of student, any many other factors. By working on this project we are hoping to develop more understanding about the factors which can impact the performance of a student.

$$
\\
$$

# 2. _Data_

This data is from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/320/student+performance). There are 649 rows instances and 30 features in the dataset. Below are details of each feature

1. school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) [Qualitative]

2. sex - student's sex (binary: 'F' - female or 'M' - male) [Qualitative]

3. age - student's age (numeric: from 15 to 22)

4. address - student's home address type (binary: 'U' - urban or 'R' - rural) [Qualitative]

5. famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3) [Qualitative]

6. Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart) [Qualitative]

7. Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education) [Qualitative]

8. Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 -  higher education) [Qualitative]

9. Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') [Qualitative]

10. Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') [Qualitative]

11. reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') [Qualitative]

12. guardian - student's guardian (nominal: 'mother', 'father' or 'other') [Qualitative]

13. traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour) [Qualitative]

14. studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) [Qualitative]

15. failures - number of past class failures (numeric: n if 1<=n<3, else 4) [Qualitative]

16. schoolsup - extra educational support (binary: yes or no) [Qualitative]

17. famsup - family educational support (binary: yes or no) [Qualitative]

18. paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) [Qualitative]

19. activities - extra-curricular activities (binary: yes or no) [Qualitative]

20. nursery - attended nursery school (binary: yes or no) [Qualitative]

21. higher - wants to take higher education (binary: yes or no) [Qualitative]

22. internet - Internet access at home (binary: yes or no) [Qualitative]

23. romantic - with a romantic relationship (binary: yes or no) [Qualitative]

24. famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) [Qualitative]

25. freetime - free time after school (numeric: from 1 - very low to 5 - very high) [Qualitative]

26. goout - going out with friends (numeric: from 1 - very low to 5 - very high) [Qualitative]

27. Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) [Qualitative]

28. Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) [Qualitative]

29. health - current health status (numeric: from 1 - very bad to 5 - very good) [Qualitative]

30. absences - number of school absences (numeric: from 0 to 93) [Quantitative]

31. G1 - first period grade (numeric: from 0 to 20) [Quantitative]

32. G2 - second period grade (numeric: from 0 to 20) [Quantitative]

33. G3 - final grade (numeric: from 0 to 20, output target) [Quantitative]


**G3 (Final grade) is the dependent variable for our model.**

>NOTE: We need to convert the qualitative variable from numeric to string

$$
\\
$$



```{r eval=TRUE, echo=FALSE}

# Code to merge student-por.csv and student-mat.csv

studentDataset_por = read.csv("https://raw.githubusercontent.com/prs-sharma9/DATA-603_PROJECT/main/dataset/student-por.csv", sep = ";")
studentDataset_mat = read.csv("https://raw.githubusercontent.com/prs-sharma9/DATA-603_PROJECT/main/dataset/student-mat.csv", sep = ";")

studentDataset = rbind(studentDataset_por, studentDataset_mat)
studentDataset = rbind(studentDataset_por, studentDataset_mat)
```

```{r}

# Converting the data type of factor variable from int to char

studentDataset$Medu= as.character(studentDataset$Medu)
studentDataset$traveltime= as.character(studentDataset$traveltime)
studentDataset$Fedu= as.character(studentDataset$Fedu)
studentDataset$studytime= as.character(studentDataset$studytime)
studentDataset$famrel= as.character(studentDataset$famrel)
studentDataset$freetime= as.character(studentDataset$freetime)
studentDataset$goout= as.character(studentDataset$goout)
studentDataset$Walc= as.character(studentDataset$Walc)
studentDataset$Dalc= as.character(studentDataset$Dalc)
studentDataset$health= as.character(studentDataset$health)

head(studentDataset, 3)
```



# 3. _Methodology_

Below is the outline of the steps we are going to perform in our analysis:

1. First Build a full additive model.

2. We will apply some model selection technique to come up with the best additive model.

2. Based on p-value (assuming $\alpha = 0.05$) we will drop variable which are non-significant.

3. Perform partial F-test to verify that dropped variables are indeed non-significant.

4. Provide interpretation for the best additive model to predict our dependent variable (G3 - Final Grades).

5. Based on our best additive model, we will check for interaction between the variables.

6. Using p-value (assuming $\alpha = 0.05$), we will drop the non-significant interaction terms.

7. Use partial F-test and analysis of Variance to verify the usability of our best interaction model.

8. Provide interpretation of our best interaction model.

9. Then we will check if we can include any higher order term in our model (Moving towards Higher order multiple regression model).

10. Verify the significance of higher order terms using p-value (assuming $\alpha = 0.05$).

11. Once we have done all our analysis we will try to define our best regression model (liner or higher order) to predict our dependent variable (G3 - Final Grades).

12. Using our final regression model, we will start checking the regression assumptions.

13. Linearity Assumption.

14. Independence Assumption.

15. Normality Assumption.

16. Multi-collinearity.

17. Outliers.

$$
\\
$$

Starting our analysis with building additive model, then we will try to include interaction terms and higher order terms in our model.

## 3.1 _Full Additive Model_

Creating full additive model: 

```{r}
studentPerformance_fm = lm(G3 ~(school+sex+age+address+famsize+Pstatus+Medu+
                                  Fedu+Mjob+Fjob+reason+guardian+traveltime+studytime+
                                  failures+schoolsup+famsup+activities+nursery+higher+internet+
                                  romantic+famrel+freetime+
                                  goout+Dalc+Walc+health+absences), 
                           data = studentDataset)

summary(studentPerformance_fm)
```

```{r}
# Multicolinearity Test
imcdiag(studentPerformance_fm, method="VIF")
```


$$
\\
$$

_Comments:_ From the summary of the full model we can see that many of the variables are non-significant. We can apply some techniques for model selection to get significant parameters.


## 3.2 _Model Selection_

Using Stepwise forward selection procedure to get the significant parameters:

$$
\\
$$


```{r}
studentPerformance_Forward_subsets = ols_step_forward_p(studentPerformance_fm,p_val = 0.05, details = FALSE)
student_performance_forwardMdl = studentPerformance_Forward_subsets$model
summary(student_performance_forwardMdl)
#studentPerformance_Forward_subsets$metric
```



>Using the result of forward selection process and dropping non-significant variables, the best additive model is:


```{r}
student_performance_addMdl = lm(G3 ~ (failures+higher+studytime+schoolsup+Dalc+health+romantic+famsize+
                                      +goout), data = studentDataset)
summary(student_performance_addMdl)
```


$$
\\
$$

**Our best additive model:**


$\widehat {G3}$ = 10.68826-1.82887(failures)+1.54720(higheryes)+0.26960(studytime2)+1.18603(studytime3)

                      +0.62526(studytime4)-1.16127(schoolsupyes)-0.73721(Dalc2)-0.14301(Dalc3)

                      -1.65491(Dalc4)+0.11089(Dalc5)-0.66440(health2)-1.21726(health3)-0.74820(health4)

                      -0.99991(health5)-0.59306(romanticyes)+0.48561(famsizeLE3)+1.15597(goout2)+0.67978(goout3)+0.38442(goout4)

                      +0.08698(goout5)



# Add some comment

## 3.3 _Interaction Model_

Using our best additive model, we can check for interaction term:

```{r}
student_performance_intMdl1 = lm(G3 ~ (failures+higher+studytime+schoolsup+Dalc+health+romantic+famsize+
                                      +goout)^2, data = studentDataset)
summary(student_performance_intMdl1)
```

$$
\\
$$

In the summary of our full interaction model the interaction term _Dacl:goout and studytime:Dacl_ has some NA entries, it simply mean that there is not enough data to generate values for those interaction. So we will drop these interaction completely.


Our interaction model will be:


```{r}
student_performance_intMdl2 = lm(G3 ~ (failures+higher+studytime+schoolsup+Dalc+health+romantic+famsize+
                                      +goout+failures:schoolsup+studytime:health+studytime:goout
                                      +schoolsup:health+schoolsup:goout), 
                                 data = studentDataset)
summary(student_performance_intMdl2)
```


## 3.4 _Comparing Additive and Interaction model_

Comparing our additive and interaction models to see which one is better. To verify this we can use partial F-test using below hypothesis:


$H_0$ : All the interaction coefficients are not significant
$H_a$ : At least one of the interaction coefficient is significant



```{r eval=TRUE, echo=FALSE}
anove_addVsint = anova(student_performance_addMdl, student_performance_intMdl2)
```


| Source of variation | DF  |  Sum of Square  |  Mean Square  |  F-Statistic|
|:-------------------:|:---:|:---------------:|:-------------:|:-----------:|
|   Regression        | 33  |    740.32       |   22.43394    |  1.9433     |
|   Residual          | 990 |    11429        |   11.54444    |             |
|   Total             | 1023|    12169        |               |             |


The p-value from our ANOVA test is **0.001228** which is less than our assumed $\alpha=0.05$ hence we can reject our $H_0$ and conclude that at least one of our interaction coefficient is significant, and 


## 3.5. _Higher Order Model_

Since we have finalized our interaction model, we can check if there are any variable (quantitative variable) for which we need to add higher order terms. For this analysis we will use pairs plot.
In our final interaction model we have only one quantitative variable (i.e. failures).

```{r}
pairs(~G3 + failures, data = studentDataset, panel = panel.smooth)
```


Though there no clear visual indication that we should add higher order term for our variable, but we can still try by adding the higher order term and check the significance.

```{r}
student_performance_intMdl3 = lm(G3 ~ (failures+I(failures^2)+higher+studytime+schoolsup+Dalc+health+romantic+famsize+
                                      +goout+failures:schoolsup+studytime:health+studytime:goout
                                      +schoolsup:health+schoolsup:goout), 
                                 data = studentDataset)
summary(student_performance_intMdl3)
```


$\widehat{G3}$ = 8.67068((Intercept)) -3.84533(failures) +0.757609(I(failures^2)) +1.49398(higheryes) 

                      +2.5596(studytime2) +7.40096(studytime3) -1.7682(studytime4) +0.454711(schoolsupyes) 

                      -0.786275(Dalc2) +0.106775(Dalc3) -1.37676(Dalc4) +0.209721(Dalc5) +0.779774(health2) 

                      -0.904056(health3) +0.246689(health4) -0.438419(health5) -0.540209(romanticyes) 

                      +0.595347(famsizeLE3) +2.62894(goout2) +2.09955(goout3) +2.32829(goout4) 

                      +1.767(goout5) +1.46286(failures:schoolsupyes) -2.21384(studytime2:health2) 

                      -1.49872(studytime3:health2) +0.227675(studytime4:health2) -0.570268(studytime2:health3) 

                      -1.94258(studytime3:health3) -1.15228(studytime4:health3) -1.40046(studytime2:health4) 

                      -2.73769(studytime3:health4) -2.13216(studytime4:health4) -0.62032(studytime2:health5) 

                      -2.35092(studytime3:health5) -0.214397(studytime4:health5) -1.12391(studytime2:goout2) 

                      -4.23562(studytime3:goout2) +4.98029(studytime4:goout2) -1.32132(studytime2:goout3) 

                      -3.90003(studytime3:goout3) +3.37694(studytime4:goout3) -1.88884(studytime2:goout4) 

                      -5.1949(studytime3:goout4) +1.88472(studytime4:goout4) -1.75839(studytime2:goout5) 

                      -3.27269(studytime3:goout5) +2.39711(studytime4:goout5) +2.3211(schoolsupyes:health2) 

                      +2.38419(schoolsupyes:health3) +2.93603(schoolsupyes:health4) +2.92593(schoolsupyes:health5) 

                      -5.59801(schoolsupyes:goout2) -4.50439(schoolsupyes:goout3) -3.8287(schoolsupyes:goout4) 

                      -5.54827(schoolsupyes:goout5)


$$
\\
$$

## 3.6 _Assumption Verification_

Our model building process is based on some assumption about the data. Hence it is very important to verify that these assumption are met or not. If our data do not fit the assumptions then the model we developed cannot be used for prediction.

### 3.6.1 Linearity Assumption

The linear regression model we build is based on the assumption that their is a linear relation between predictors and response variable. To confirm the linear relation we can use Residula plot.


```{r eval=TRUE, echo=FALSE}
ggplot(student_performance_intMdl3, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)+
ggtitle("Residual plot: Residual vs Fitted values")
```

>Comment: The residual vs fitted plot show a straight line which indicate that our linearity assumption is met.


### 3.6.2 Equal Variance Assumption

Another important assumption for our liner regression model is that the error term has a constant variance (**homoscedasticity**). To verify the homoscedasticity assumption we can again use the residual vs fitted plot and check if there is any patter.


```{r eval=TRUE, echo=FALSE}
ggplot(student_performance_intMdl3, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```


To confirm homoscedasticity we can perform the Breusch-Pagan Test (bptest) using below hypothesis:

$H_0$ : Hetroscedasticity is not present (error term has comman variance)
$H_a$ : Hetroscedasticity is present (error term do not have comman variance)


```{r}
bptest(student_performance_intMdl3)  # It doesn't have heteroscedasticity.
```

>Comment: Since the p-value is 0.3235 higher then our assumed $\alpha=0.05$ we cannot reject the $H_0$ and conclude that our model meets the assumption of common variance.


### 3.6.3 Normality Assumption


```{r eval=TRUE, echo=FALSE}
require(gridExtra)

p1=ggplot(data=studentDataset, aes(residuals(student_performance_intMdl3))) +
geom_histogram(breaks = seq(-6,6,by=1), col="green3", fill="green4") +
labs(title="Histogram for residuals") +
labs(x="residuals", y="Count")

p2=ggplot(studentDataset, aes(sample=student_performance_intMdl3$residuals)) +
stat_qq() +
stat_qq_line()+
ggtitle("Q-Q Plot")

grid.arrange(p1, p2, ncol=2)
```



```{r}
shapiro.test(residuals(student_performance_intMdl3))  # We don't have normality
```
### 3.6.4 Independence Assumption

Check if we need to add anything.

### 3.6.5 Multicollinearity

```{r}
#finalMdl = lm(G3 ~ (failures+higher+studytime+schoolsup+Dalc+health+romantic+famsize+
#                                      +goout+failures:schoolsup+studytime:health+studytime:goout
#                                      +schoolsup:health+schoolsup:goout), 
#                                 data = studentDataset)

#finalMdl = lm(G3 ~ (failures+higher+schoolsup+Dalc+health+romantic+famsize+
#                                      +goout+failures:schoolsup+schoolsup:health+schoolsup:goout), 
#                                data = studentDataset)

finalMdl = lm(G3 ~ (failures+higher+schoolsup+Dalc+health+romantic+famsize+
                                      +goout+studytime), 
                                 data = studentDataset)
imcdiag(finalMdl, method="VIF")
summary(finalMdl)
```


### 3.6.6 Outlier

```{r}
plot(student_performance_intMdl3,which=5)

studentDataset[cooks.distance(student_performance_intMdl3)>0.5,]
```



# 4. _Conclusion_

state final model
give interpretation

# 5. _Discussion_

Future course of action


# Appendix


***

$$
END
$$




```{r}
studentPerformance_p1 = lm(G3 ~ (sex+age+address+famsize+Pstatus+Medu+Fedu+traveltime+studytime+
                                   failures+schoolsup+famsup+activities+higher+internet+romantic+
                                   famrel+freetime+goout+Dalc+Walc+health+absences), 
                           data = studentDataset)

summary(studentPerformance_p1)
```


```{r}
subsets = ols_step_forward_p(studentPerformance_fm,p_val = 0.05, details = FALSE)
forwardMdl = subsets$model
summary(forwardMdl)
```

```{r}
bestSubsetMdl = ols_step_best_subset(forwardMdl, details = FALSE)
(bestSubsetMdl$metrics)
```

```{r}
studentPerformance_p2 = lm(G3 ~ (failures+higher+schoolsup+school+romantic+studytime+address+Medu+Dalc+famsize+freetime+goout+health), 
                           data = studentDataset)

summary(studentPerformance_p2)
```

```{r}
studentPerformance_p3 = lm(G3 ~ (failures+higher+schoolsup+school+romantic+studytime+address+Dalc+famsize+freetime+goout+health), 
                           data = studentDataset)

summary(studentPerformance_p3)
```

```{r}
studentPerformance_p4 = lm(G3 ~ (failures+higher+schoolsup+school+romantic+studytime+address+Dalc+famsize+goout+health), 
                           data = studentDataset)

summary(studentPerformance_p4)
```

```{r}
forwardMdl_int = lm(G3 ~ (failures+higher+schoolsup+school+romantic+studytime+address+Dalc+famsize+goout+health)^2, data = studentDataset)
summary(forwardMdl_int)
```


```{r}
forwardMdl_int1 = lm(G3 ~ (failures+higher+schoolsup+school+romantic+studytime+address+Dalc+famsize+goout+health+failures:schoolsup+higher:studytime+schoolsup:goout+schoolsup:health+school:studytime+school:goout+studytime:goout+studytime:health+Dalc:goout), data = studentDataset)
summary(forwardMdl_int1)
```

```{r}
anova(studentPerformance_p4,forwardMdl_int1)
```


### Assumption code


```{r}
#Linearity Assumption
library(ggplot2)
ggplot(forwardMdl_int1, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)
```

```{r}
# Equal Variance Assumption

ggplot(forwardMdl_int1, aes(x=.fitted, y=.resid)) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth(colour = "green4")+
ggtitle("Residual plot: Residual vs Fitted values")
```

```{r}
ggplot(forwardMdl_int1, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth( colour = "green4")+
ggtitle("Scale-Location plot : Standardized Residual vs Fitted values")
```
```{r}
library(lmtest)

bptest(forwardMdl_int1)  # It doesn't have heteroscedasticity.
```

```{r}
# Normality Assumption

ggplot(data=studentDataset, aes(residuals(forwardMdl_int1))) +
geom_histogram(breaks = seq(-6,6,by=1), col="green3", fill="green4") +
labs(title="Histogram for residuals") +
labs(x="residuals", y="Count")
```
```{r}
ggplot(studentDataset, aes(sample=forwardMdl_int1$residuals)) +
stat_qq() +
stat_qq_line()
```
```{r}
#Testing for Normality  
shapiro.test(residuals(forwardMdl_int1))  # We don't have normality
```

```{r}
#Multicollinearity
library(mctest)
#pairs(~G3+studytime+goout+health,data=studentDataset)
```


```{r}
imcdiag(studentPerformance_fm, method="VIF")
```

```{r}
forwardMdl_int2 = lm(G3 ~ (failures+higher+schoolsup+school+romantic+studytime+address+Dalc+famsize+goout+health+failures:schoolsup+schoolsup:goout+schoolsup:health+school:studytime+school:goout+studytime:health), data = studentDataset)
summary(forwardMdl_int2)
```


```{r}

imcdiag(forwardMdl_int2, method="VIF")
```


```{r}
#Linearity Assumption
library(ggplot2)
ggplot(forwardMdl_int2, aes(x=.fitted, y=.resid)) +
geom_point() +geom_smooth()+
geom_hline(yintercept = 0)
```
```{r}
ggplot(forwardMdl_int2, aes(x=.fitted, y=sqrt(abs(.stdresid)))) +
geom_point(colour = "purple") +
geom_hline(yintercept = 0) +
geom_smooth( colour = "green4")+
ggtitle("Scale-Location plot : Standardized Residual vs Fitted values")
```
```{r}
library(lmtest)

bptest(forwardMdl_int2)  # It doesn't have heteroscedasticity.
```

```{r}
# Normality Assumption

ggplot(data=studentDataset, aes(residuals(forwardMdl_int2))) +
geom_histogram(breaks = seq(-5,5,by=1), col="green3", fill="green4") +
labs(title="Histogram for residuals") +
labs(x="residuals", y="Count")
```
```{r}
ggplot(studentDataset, aes(sample=forwardMdl_int2$residuals)) +
stat_qq() +
stat_qq_line()
```
```{r}
#Testing for Normality  
shapiro.test(residuals(forwardMdl_int2))  # We don't have normality
```
```{r}
qqnorm(studentDataset$G3)
qqline(studentDataset$G3, col="red")

```


```{r}
# Outlier

plot(forwardMdl_int2,which=5)
```
```{r}
studentDataset[cooks.distance(forwardMdl_int2)>0.5,]
```

```{r}
plot(forwardMdl_int2,pch=18,col="red",which=c(4))
```

```{r}


formula <- function(model,dep){

  coe=coefficients(model)

  b=paste(dep,'=')

  for(a in names(coe)){

    if (coe[a]>0){

      b=paste(b,' +',signif(coe[a]),'(',a,')',sep="")

    }else{

      b=paste(b,' ',signif(coe[a]),'(',a,')',sep="")

    }

  }

  print(b)

}

formula(student_performance_intMdl3,'G3')

```
```{r}
fanyi <- function(model,dep,listofcolums) {
  coe=coefficients(model)
  inter=data.frame(X=listofcolums)
  inter$B=''
  for(a in names(coe)){
    if (grepl(':',a)){
      c=strsplit(a,':')[[1]]
      c[3]=''
      c[4]=''
      if(!(c[1] %in% listofcolums)){
        for (i in listofcolums){
          if (length(grep(i,c[1])==1)){
            c[1]=paste('if ',i,' is ',str_remove(string = c[1],pattern = i),sep='')
            c[3]=i
          }
        }
      }
      if(!(c[2] %in% listofcolums)){
        for (i in listofcolums){
          if (length(grep(i,c[2])==1)){
            c[2]=paste('if ',i,' is ',str_remove(string = c[2],pattern = i),sep='')
            c[4]=i
          }
        }
      }
      if(coe[a]>0){
        inter[inter$X==c[1]|inter$X==c[3],]$B<-paste(inter[inter$X==c[1]|inter$X==c[3],]$B,'+',signif(coe[a]),'(',c[2],')',sep='')
        inter[inter$X==c[2]|inter$X==c[4],]$B<-paste(inter[inter$X==c[2]|inter$X==c[4],]$B,'+',signif(coe[a]),'(',c[1],')',sep='')
      }else{
        inter[inter$X==c[1]|inter$X==c[3],]$B<-paste(inter[inter$X==c[1]|inter$X==c[3],]$B,signif(coe[a]),'(',c[2],')',sep='')
        inter[inter$X==c[2]|inter$X==c[4],]$B<-paste(inter[inter$X==c[2]|inter$X==c[4],]$B,signif(coe[a]),'(',c[1],')',sep='')
      }
    }else{
      if (a %in% listofcolums){
        inter[inter$X==a,]$B<-paste(inter[inter$X==a,]$B,signif(coe[a]),sep = '')
      }else{
        for (i in listofcolums){
          if (length(grep(i,a)==1)){
            d=str_remove(string = a,pattern = i)
            if(coe[a]>0){
              inter[inter$X==i,]$B<-paste(inter[inter$X==i,]$B,'+',signif(coe[a]),'(if ',i,' is ',d,')',sep='')
            }else{
              inter[inter$X==i,]$B<-paste(inter[inter$X==i,]$B,signif(coe[a]),'(if ',i,' is ',d,')',sep='')
            }
          }
        }
      }
    }
    
  }
  print(inter)
  for(i in listofcolums){
    print(paste('For 1 unit increases in\'',i,'\'',dep,'increases by',inter[inter$X==i,]$B))
  }
} 


fanyi(student_performance_intMdl3,'the grade of student',c('failures','higher','study','schoolsup','Dalc','health','romantic','famsizeLE','goout'))

```


